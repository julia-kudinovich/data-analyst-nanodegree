{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3AOne+Direction&fmt=json\n",
      "2010-07\n",
      "{\n",
      "    \"artists\": [\n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "                \"name\": \"United Kingdom\", \n",
      "                \"sort-name\": \"United Kingdom\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
      "                \"name\": \"London\", \n",
      "                \"sort-name\": \"London\"\n",
      "            }, \n",
      "            \"country\": \"GB\", \n",
      "            \"disambiguation\": \"English-Irish boy band formed in 2010\", \n",
      "            \"id\": \"1a425bbd-cca4-4b2c-aeb7-71cb176c828a\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2010-07\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"One Direction\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"One Direction\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 2, \n",
      "                    \"name\": \"pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"power pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"dance-pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"pop rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"folk pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 2, \n",
      "                    \"name\": \"boy band\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"disambiguation\": \"San Francisco band\", \n",
      "            \"id\": \"d3479e62-76ac-4aec-9f95-5b222d1e26b1\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Direction\", \n",
      "            \"score\": \"69\", \n",
      "            \"sort-name\": \"Direction\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"b2bc1294-77be-4c7b-af93-9868b83b1f34\", \n",
      "                \"name\": \"Auckland\", \n",
      "                \"sort-name\": \"Auckland\"\n",
      "            }, \n",
      "            \"disambiguation\": \"New Zealand punk?\", \n",
      "            \"id\": \"a234ebb0-0399-4253-baf6-2ae0abc9d8f2\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Direction\", \n",
      "            \"score\": \"47\", \n",
      "            \"sort-name\": \"Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"cfd2b787-3656-44ce-9212-9e555b400af5\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"New Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"New Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"2524b4ea-965f-4628-b940-acd0a9210047\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Lost Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Lost Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"a32cc520-f2e6-4db2-8b61-c7baca599ef5\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Second Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Second Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"47813cd0-be78-48ab-802f-1306fc180124\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Loose Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Loose Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"75fb2c35-4f3a-4145-b948-da871176f039\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Disco Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Disco Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"9e9d35c3-ce54-4a48-b3ba-4eebdadc1ed8\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Audio Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Audio Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"6130ddae-22dd-4930-ba0f-6d59f81d3637\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2009-08\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"The Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Direction, The\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"disambiguation\": \"USA punk band\", \n",
      "            \"id\": \"d554887e-2e17-48f4-8610-624a944a9329\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"No Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"No Direction\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"punk\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"usa\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"355fa6a9-dba2-43b0-bd0f-a9e6dc14c79f\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Direction X\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Direction X\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"7c4e4dc0-6a0e-4887-b374-0cc57749f563\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Direction Indicator\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Direction Indicator\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"74c2fbfc-0ced-4f8a-9da6-7df628755757\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"No Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"No Direction\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"ce85410c-10e4-4b2e-a878-72483fcf0884\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"7th Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"7th Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"ef1b7cc0-cd26-36f4-8ea0-04d9623786c7\", \n",
      "                \"name\": \"Netherlands\", \n",
      "                \"sort-name\": \"Netherlands\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"0b565bc6-05e7-4c3a-bc28-075fabd9a8d0\", \n",
      "                \"name\": \"Maastricht\", \n",
      "                \"sort-name\": \"Maastricht\"\n",
      "            }, \n",
      "            \"country\": \"NL\", \n",
      "            \"id\": \"574236dc-aa46-4194-8200-6cb039e9ddbd\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1989\", \n",
      "                \"end\": \"2008\", \n",
      "                \"ended\": true\n",
      "            }, \n",
      "            \"name\": \"Right Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Right Direction\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"65cd1bef-0de3-439e-a2fc-a71c308c6359\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"\\u9ad8\\u67f3\\u660c\\u884c New Direction\", \n",
      "            \"score\": \"38\", \n",
      "            \"sort-name\": \"Masayuki Takayanagi New Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Masayuki \\\"Jojo\\\" Takayanagi* & New Direction For The Arts\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Masayuki \\\"Jojo\\\" Takayanagi* & New Direction For The Arts\", \n",
      "                    \"type\": null\n",
      "                }\n",
      "            ], \n",
      "            \"area\": {\n",
      "                \"id\": \"2db42837-c832-3c27-b4a3-08198f75693c\", \n",
      "                \"name\": \"Japan\", \n",
      "                \"sort-name\": \"Japan\"\n",
      "            }, \n",
      "            \"country\": \"JP\", \n",
      "            \"id\": \"8367123e-9a2b-4f1a-ad22-52dee85b144a\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"New Direction for the Arts\", \n",
      "            \"score\": \"36\", \n",
      "            \"sort-name\": \"New Direction for the Arts\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"3ccdd708-a6f2-4117-938d-4b23f045cbbb\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"The New Transit Direction\", \n",
      "            \"score\": \"34\", \n",
      "            \"sort-name\": \"New Transit Direction, The\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"71bbafaa-e825-3e15-8ca9-017dcad1748b\", \n",
      "                \"name\": \"Canada\", \n",
      "                \"sort-name\": \"Canada\"\n",
      "            }, \n",
      "            \"country\": \"CA\", \n",
      "            \"id\": \"e0027978-441c-4877-b36f-0d2833181a42\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"The Beat Direction\", \n",
      "            \"score\": \"34\", \n",
      "            \"sort-name\": \"Beat Direction, The\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"disambiguation\": \"rock\", \n",
      "            \"id\": \"dffb4132-bfd1-4c23-8128-31c23d0b6fe9\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Found My Direction\", \n",
      "            \"score\": \"34\", \n",
      "            \"sort-name\": \"Found My Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"85752fda-13c4-31a3-bee5-0e5cb1f51dad\", \n",
      "                \"name\": \"Germany\", \n",
      "                \"sort-name\": \"Germany\"\n",
      "            }, \n",
      "            \"country\": \"DE\", \n",
      "            \"disambiguation\": \"Hardcore/Gabber DJ Team\", \n",
      "            \"id\": \"06a2966d-95c1-4e20-a4d6-26e15d1d7e69\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2010\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Tensor & Re-Direction\", \n",
      "            \"score\": \"34\", \n",
      "            \"sort-name\": \"Tensor & Re-Direction\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"982461f8-d0e0-44c0-9aa3-9ce9df28c71a\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Moving in The Right Direction\", \n",
      "            \"score\": \"31\", \n",
      "            \"sort-name\": \"Moving in The Right Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"disambiguation\": \"Church choir at Lourdes\", \n",
      "            \"id\": \"c2485094-3ac9-4632-b8d8-aacae7df3b6f\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Chorale et assembl\\u00e9e sous la direction de Jean Zune\", \n",
      "            \"score\": \"30\", \n",
      "            \"sort-name\": \"Chorale et assembl\\u00e9e sous la direction de Jean Zune\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"043456b7-0156-4a8d-9591-3018af75ced1\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Orchestre Symphonique National direction Wal-Berg\", \n",
      "            \"score\": \"28\", \n",
      "            \"sort-name\": \"Orchestre Symphonique National direction Wal-Berg\"\n",
      "        }\n",
      "    ], \n",
      "    \"count\": 1719, \n",
      "    \"created\": \"2017-02-10T19:59:16.401Z\", \n",
      "    \"offset\": 0\n",
      "}\n",
      "\n",
      "ARTIST:\n",
      "{\n",
      "    \"area\": {\n",
      "        \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "        \"name\": \"United States\", \n",
      "        \"sort-name\": \"United States\"\n",
      "    }, \n",
      "    \"country\": \"US\", \n",
      "    \"disambiguation\": \"San Francisco band\", \n",
      "    \"id\": \"d3479e62-76ac-4aec-9f95-5b222d1e26b1\", \n",
      "    \"life-span\": {\n",
      "        \"ended\": null\n",
      "    }, \n",
      "    \"name\": \"Direction\", \n",
      "    \"score\": \"69\", \n",
      "    \"sort-name\": \"Direction\", \n",
      "    \"type\": \"Group\"\n",
      "}\n",
      "requesting http://musicbrainz.org/ws/2/artist/d3479e62-76ac-4aec-9f95-5b222d1e26b1?fmt=json&inc=releases\n",
      "\n",
      "ONE RELEASE:\n",
      "{\n",
      "  \"barcode\": \"881034552525\", \n",
      "  \"country\": \"US\", \n",
      "  \"date\": \"2010-10-19\", \n",
      "  \"disambiguation\": \"\", \n",
      "  \"id\": \"09394d29-4854-48cc-b93e-77385bb0a03e\", \n",
      "  \"packaging\": null, \n",
      "  \"packaging-id\": null, \n",
      "  \"quality\": \"normal\", \n",
      "  \"release-events\": [\n",
      "    {\n",
      "      \"area\": {\n",
      "        \"disambiguation\": \"\", \n",
      "        \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "        \"iso-3166-1-codes\": [\n",
      "          \"US\"\n",
      "        ], \n",
      "        \"name\": \"United States\", \n",
      "        \"sort-name\": \"United States\"\n",
      "      }, \n",
      "      \"date\": \"2010-10-19\"\n",
      "    }\n",
      "  ], \n",
      "  \"status\": \"Official\", \n",
      "  \"status-id\": \"4e304316-386d-3409-af2e-78857eec5cfe\", \n",
      "  \"text-representation\": {\n",
      "    \"language\": \"eng\", \n",
      "    \"script\": \"Latn\"\n",
      "  }, \n",
      "  \"title\": \"Blue Room in San Francisco\"\n",
      "}\n",
      "\n",
      "ALL TITLES:\n",
      "Blue Room in San Francisco\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    # This is the main function for making queries to the musicbrainz API.\n",
    "    # A json document should be returned by the query.\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print \"requesting\", r.url\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    # This adds an artist name to the query parameters before making\n",
    "    # an API call to the function above.\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    # After we get our output, we can format it to be more readable\n",
    "    # by using this function.\n",
    "    if type(data) == dict:\n",
    "        print json.dumps(data, indent=indent, sort_keys=True)\n",
    "    else:\n",
    "        print data\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Modify the function calls and indexing below to answer the questions on\n",
    "    the next quiz. HINT: Note how the output we get from the site is a\n",
    "    multi-level JSON document, so try making print statements to step through\n",
    "    the structure one level at a time or copy the output to a separate output\n",
    "    file.\n",
    "    '''\n",
    "    \n",
    "    #q1\n",
    "    '''\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"First Aid Kit\")\n",
    "    count = 0\n",
    "    for i in results['artists']:\n",
    "        if i['name'] == 'First Aid Kit':\n",
    "            count+=1\n",
    "    print count\n",
    "\n",
    "    #q2\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Queen\")\n",
    "    print results['artists'][2]['begin-area']['name']\n",
    " \n",
    "    \n",
    "    #q3\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"The Beatles\")\n",
    "    for i in results['artists'][0]['aliases']:\n",
    "        if i['locale']=='es':\n",
    "            print i['name']\n",
    "\n",
    "  \n",
    "    #q4\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")\n",
    "    print results['artists'][4]['disambiguation']\n",
    "    \n",
    "    \n",
    "    #q5\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"One Direction\")\n",
    "    print results['artists'][0]['life-span']['begin']\n",
    "     \n",
    "    '''\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")\n",
    "    \n",
    "    pretty_print(results)\n",
    "\n",
    "    artist_id = results[\"artists\"][1][\"id\"]\n",
    "    print \"\\nARTIST:\"\n",
    "    pretty_print(results[\"artists\"][1])\n",
    "\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    releases = artist_data[\"releases\"]\n",
    "    print \"\\nONE RELEASE:\"\n",
    "    pretty_print(releases[0], indent=2)\n",
    "    release_titles = [r[\"title\"] for r in releases]\n",
    "\n",
    "    print \"\\nALL TITLES:\"\n",
    "    for t in release_titles:\n",
    "        print t\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '2013_ERCOT_Hourly_Load_Data.xls.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-a015556204a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-a015556204a3>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mopen_zip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0msave_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-a015556204a3>\u001b[0m in \u001b[0;36mopen_zip\u001b[0;34m(datafile)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mopen_zip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[1;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{0}.zip'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmyzip\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mmyzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\zipfile.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0mmodeDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'r'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'r+b'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '2013_ERCOT_Hourly_Load_Data.xls.zip'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "'''\n",
    "\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"2013_Max_Loads.csv\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    data = [['Station', 'Year', 'Month', 'Day', 'Hour', 'Max Load' ]]\n",
    "   \n",
    "    for col in range(sheet.ncols):\n",
    "        if sheet.cell_value(0,col) in ['COAST', 'EAST', 'FAR_WEST', 'NORTH', 'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']:\n",
    "            station = sheet.cell_value(0,col)\n",
    "            max_value =  max(sheet.col_values(col,  start_rowx=1))\n",
    "               \n",
    "            max_index = sheet.col_values(col,  start_rowx=1).index(max_value)+1\n",
    "            max_time = xlrd.xldate_as_tuple(sheet.cell_value(max_index, 0), 0)\n",
    "    \n",
    "            data.append([station, max_time[0], max_time[1], max_time[2], max_time[3],max_value])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_file(data, filename):\n",
    "    with open(filename,'wb') as resultFile:\n",
    "        wr = csv.writer(resultFile ,delimiter='|')\n",
    "        wr.writerows(data)\n",
    "\n",
    "    \n",
    "def test():\n",
    "    open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            print line\n",
    "            station = line['Station']\n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert max_answer == max_line\n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        assert ans[station][field] == line[field]\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "        assert number_of_rows == 8\n",
    "\n",
    "        # Check Station Names\n",
    "        assert set(stations) == set(correct_stations)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article_overview(kind, period):\n",
    "    data = get_from_file(kind, period)\n",
    "    titles = []\n",
    "    urls =[]\n",
    "    # titles = [{x['section']:x['title']} for x in data]\n",
    "    #urls = [z['url'] for x in data for y in x['media'] for z in y['media-metadata'] if z['format']=='Standard Thumbnail']\n",
    "    for i in data:\n",
    "        titles.append({i['section']: i['title']})\n",
    "        for media in i['media']:\n",
    "            for meta in  media['media-metadata']:\n",
    "                if meta['format']=='Standard Thumbnail':\n",
    "                    urls.append(meta['url'])\n",
    "\n",
    "\n",
    "    return (titles, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'exampleResearchArticle.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-b7215566c0a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-b7215566c0a7>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0msolution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'fnm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Omer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'snm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Mei-Dan'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'email'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'omer@extremegate.com'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'fnm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Mike'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'snm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Carmont'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'email'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'mcarmont@hotmail.com'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'fnm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Lior'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'snm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Laver'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'email'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'laver17@gmail.com'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'fnm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Meir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'snm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Nyska'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'email'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'nyska@internet-zahav.net'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'fnm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Hagay'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'snm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Kammar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'email'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'kammarh@gmail.com'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'fnm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Gideon'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'snm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Mann'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'email'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'gideon.mann.md@gmail.com'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'fnm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Barnaby'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'snm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Clarck'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'email'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'barns.nz@gmail.com'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'fnm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Eugene'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'snm'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Kots'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'email'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'eukots@gmail.com'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_authors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-b7215566c0a7>\u001b[0m in \u001b[0;36mget_root\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\xml\\etree\\ElementTree.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\xml\\etree\\ElementTree.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0mclose_source\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mclose_source\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'exampleResearchArticle.xml'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# Your task here is to extract data from xml on authors of an article\n",
    "# and add it to a list, one item for an author.\n",
    "# See the provided data structure for the expected format.\n",
    "# The tags for first name, surname and email should map directly\n",
    "# to the dictionary keys\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None,\n",
    "                \"insr\": []\n",
    "        }\n",
    "\n",
    "        data[\"fnm\"] = author.find('fnm').text\n",
    "        data[\"snm\"] = author.find('snm').text\n",
    "        data[\"email\"] = author.find('email').text\n",
    "        for i in author.findall('insr'):\n",
    "            data[\"insr\"].append(i.attrib[\"iid\"])\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "def test():\n",
    "    solution = [{'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'}, {'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'}, {'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'}, {'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'}, {'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'}, {'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'}, {'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'}, {'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "    \n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"fnm\"] == solution[1][\"fnm\"]\n",
    "\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-50-3ac628d2ec2f>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-50-3ac628d2ec2f>\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    return data\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Please note that the function 'make_request' is provided for your reference only.\n",
    "# You will not be able to to actually use it from within the Udacity web UI.\n",
    "# Your task is to process the HTML using BeautifulSoup, extract the hidden\n",
    "# form field values for \"__EVENTVALIDATION\" and \"__VIEWSTATE\" and set the appropriate\n",
    "# values in the data dictionary.\n",
    "# All your changes should be in the 'extract_data' function\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "html_page = \"page_source.html\"\n",
    "#s = requests.Session()\n",
    "#r = s.get(\"http://www.transtats.bts.gov/Dara_Elements.aspx?Data=2\")\n",
    "#soup = BeautifulSoup(r.text)\n",
    "\n",
    "def extract_data(page):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    with open(page, \"r\") as html:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        ev = soup.find(id=\"__EVENTVALIDATION\")\n",
    "        data[\"eventvalidation\"] = ev[\"value\"]\n",
    "\n",
    "        vs = soup.find(id=\"__VIEWSTATE\")\n",
    "        data[\"viewstate\"] = vs[\"value\"]\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        carrier_list = soup.find(id=\"CarrierList\").find_all('option')\n",
    "        for i in carrier_list:\n",
    "            if \"All\" not in i[\"value\"]:\n",
    "                print  i[\"value\"]\n",
    "                data.append(i[\"value\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "\n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        rows = soup.find_all('tr', 'dataTDRight')\n",
    "        for row in rows:\n",
    "            tds = row.find_all('td')\n",
    "            if tds[1].text != 'TOTAL':\n",
    "                info['year'] = int(tds[0].text)\n",
    "                info['month'] = int(tds[1].text)\n",
    "                info['flights'] = {'domestic': int(tds[2].text.replace(',','')), \n",
    "                                   'international': int(tds[3].text.replace(',',''))}\n",
    "                data.append(info)\n",
    "        \n",
    "    return data\n",
    "\n",
    "'''\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "    #f = open(\"virgin_and_logan_airport.html\",\"w\")\n",
    "    #f.write(r.text)\n",
    "    return r.text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_data(html_page)\n",
    "    assert data[\"eventvalidation\"] != \"\"\n",
    "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
    "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
    "\n",
    "    \n",
    "test()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'patent.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-b46a7c5b259e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Could not find file {}. Check if the filename is correct!\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-b46a7c5b259e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0msplit_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATENTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-b46a7c5b259e>\u001b[0m in \u001b[0;36msplit_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_in\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf_in\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}-{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mw_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'patent.data'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# So, the problem is that the gigantic file is actually not a valid XML, because\n",
    "# it has several root elements, and XML declarations.\n",
    "# It is, a matter of fact, a collection of a lot of concatenated XML documents.\n",
    "# So, one solution would be to split the file into separate documents,\n",
    "# so that you can process the resulting files as valid XML documents.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import itertools\n",
    "\n",
    "PATENTS = 'patent.data'\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "def split_file(filename):\n",
    "    \"\"\"\n",
    "    Split the input file into separate files, each containing a single patent.\n",
    "    As a hint - each patent declaration starts with the same line that was\n",
    "    causing the error found in the previous exercises.\n",
    "    \n",
    "    The new files should be saved with filename in the following format:\n",
    "    \"{}-{}\".format(filename, n) where n is a counter, starting from 0.\n",
    "\"\"\"\n",
    "\n",
    "    n = 0\n",
    "    \n",
    "    with open(filename,'r') as f_in:\n",
    "            for i in f_in:\n",
    "                with open(\"{}-{}\".format(filename, n), 'a') as w_file:\n",
    "                    if not 'patent-grant>' in i:\n",
    "                        w_file.write(i)\n",
    "                    else:\n",
    "                        w_file.write(i)\n",
    "                        n+=1\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def test():\n",
    "    split_file(PATENTS)\n",
    "    for n in range(4):\n",
    "        try:\n",
    "            fname = \"{}-{}\".format(PATENTS, n)\n",
    "            f = open(fname, \"r\")\n",
    "            \n",
    "            if not f.readline().startswith(\"<?xml\"):\n",
    "                print \"You have not split the file {} in the correct boundary!\".format(fname)\n",
    "            f.close()\n",
    "        except:\n",
    "            print \"Could not find file {}. Check if the filename is correct!\".format(fname)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-52-e8134209e820>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-e8134209e820>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Example Using Our Blueprint\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Example Using Our Blueprint\n",
    "The OSM file used is an abbreviated version of the Chicago Mapzen file found here. The complete OSM file is large (~1.8 GB), but a sample can be made using the sampling code in the Project Prep.\n",
    "\n",
    "The code used in the video:\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "osm_file = open(\"chicago.osm\", \"r\")\n",
    "\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(int)\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "\n",
    "        street_types[street_type] += 1\n",
    "\n",
    "def print_sorted_dict(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print \"%s: %d\" % (k, v) \n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit():\n",
    "    for event, elem in ET.iterparse(osm_file):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])    \n",
    "    print_sorted_dict(street_types)    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    audit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'autos.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-c118c7f98ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-c118c7f98ce1>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mprocess_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mINPUT_FILE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOUTPUT_GOOD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOUTPUT_BAD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-c118c7f98ce1>\u001b[0m in \u001b[0;36mprocess_file\u001b[0;34m(input_file, output_good, output_bad)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mgood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mbad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'autos.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your task is to check the \"productionStartYear\" of the DBPedia autos datafile for valid values.\n",
    "The following things should be done:\n",
    "- check if the field \"productionStartYear\" contains a year\n",
    "- check if the year is in range 1886-2014\n",
    "- convert the value of the field to be just a year (not full datetime)\n",
    "- the rest of the fields and values should stay the same\n",
    "- if the value of the field is a valid year in the range as described above,\n",
    "  write that line to the output_good file\n",
    "- if the value of the field is not a valid year as described above, \n",
    "  write that line to the output_bad file\n",
    "- discard rows (neither write to good nor bad) if the URI is not from dbpedia.org\n",
    "- you should use the provided way of reading and writing data (DictReader and DictWriter)\n",
    "  They will take care of dealing with the header.\n",
    "\n",
    "You can write helper functions for checking the data and writing the files, but we will call only the \n",
    "'process_file' with 3 arguments (inputfile, output_good, output_bad).\n",
    "\"\"\"\n",
    "import csv\n",
    "import pprint\n",
    "\n",
    "INPUT_FILE = 'autos.csv'\n",
    "OUTPUT_GOOD = 'autos-valid.csv'\n",
    "OUTPUT_BAD = 'FIXME-autos.csv'\n",
    "\n",
    "def process_file(input_file, output_good, output_bad):\n",
    "    good = []\n",
    "    bad = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        for row in reader:\n",
    "            if \"http://dbpedia.org\" in row[\"URI\"]:\n",
    "                year = row[\"productionStartYear\"][:4]\n",
    "                if year == \"NULL\":\n",
    "                    bad.append(row)\n",
    "                elif 1885 < int(year) < 2015:\n",
    "                    row[\"productionStartYear\"] = int(year)\n",
    "                    good.append(row)\n",
    "                else:\n",
    "                    bad.append(row)\n",
    "\n",
    "\n",
    "    # This is just an example on how you can use csv.DictWriter\n",
    "    # Remember that you have to output 2 files\n",
    "    with open(output_good, \"w\") as g:\n",
    "        writer = csv.DictWriter(g, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in good:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "    with open(output_bad, \"w\") as b:\n",
    "        writer = csv.DictWriter(b, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in bad:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def test():\n",
    "\n",
    "    process_file(INPUT_FILE, OUTPUT_GOOD, OUTPUT_BAD)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'cities.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-6dbc659f1e95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-6dbc659f1e95>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mfieldtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudit_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCITIES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFIELDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfieldtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-6dbc659f1e95>\u001b[0m in \u001b[0;36maudit_file\u001b[0;34m(filename, fields)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maudit_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mfieldtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'cities.csv'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up. In the first exercise we want you to audit\n",
    "the datatypes that can be found in some particular fields in the dataset.\n",
    "The possible types of values can be:\n",
    "- NoneType if the value is a string \"NULL\" or an empty string \"\"\n",
    "- list, if the value starts with \"{\"\n",
    "- int, if the value can be cast to int\n",
    "- float, if the value can be cast to float, but CANNOT be cast to int.\n",
    "   For example, '3.23e+07' should be considered a float because it can be cast\n",
    "   as float but int('3.23e+07') will throw a ValueError\n",
    "- 'str', for all other values\n",
    "\n",
    "The audit_file function should return a dictionary containing fieldnames and a \n",
    "SET of the types that can be found in the field. e.g.\n",
    "{\"field1\": set([type(float()), type(int()), type(str())]),\n",
    " \"field2\": set([type(str())]),\n",
    "  ....\n",
    "}\n",
    "The type() function returns a type object describing the argument given to the \n",
    "function. You can also use examples of objects to create type objects, e.g.\n",
    "type(1.1) for a float: see the test function below for examples.\n",
    "\n",
    "Note that the first three rows (after the header row) in the cities.csv file\n",
    "are not actual data points. The contents of these rows should note be included\n",
    "when processing data types. Be sure to include functionality in your code to\n",
    "skip over or detect these rows.\n",
    "\"\"\"\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "\n",
    "FIELDS = [\"name\", \"timeZone_label\", \"utcOffset\", \"homepage\", \"governmentType_label\",\n",
    "          \"isPartOf_label\", \"areaCode\", \"populationTotal\", \"elevation\",\n",
    "          \"maximumElevation\", \"minimumElevation\", \"populationDensity\",\n",
    "          \"wgs84_pos#lat\", \"wgs84_pos#long\", \"areaLand\", \"areaMetro\", \"areaUrban\"]\n",
    "\n",
    "def is_int(v):\n",
    "    try:\n",
    "        int(v)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_float(v):\n",
    "    try:\n",
    "        float(v)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def audit_file(filename, fields):\n",
    "    fieldtypes = {}\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        for filed in fields:\n",
    "            fieldtypes[filed] = set()\n",
    "        for row in reader:\n",
    "            if \"http://dbpedia.org\" in row[\"URI\"]:\n",
    "                for f in fields:\n",
    "                    if row[f] == \"NULL\" or row[f]==\"\":\n",
    "                        fieldtypes[f].add(type(None))\n",
    "                    elif row[f][0] =='{':\n",
    "                        fieldtypes[f].add(type([]))\n",
    "                    elif is_int(row[f]):\n",
    "                        fieldtypes[f].add(type(1))\n",
    "                    elif is_float(row[f]):\n",
    "                        fieldtypes[f].add(type(1.1))\n",
    "                    else:\n",
    "                        fieldtypes[f].add(type(\"string\"))\n",
    "                    \n",
    "    print fieldtypes\n",
    "    return fieldtypes\n",
    "\n",
    "\n",
    "def test():\n",
    "    fieldtypes = audit_file(CITIES, FIELDS)\n",
    "\n",
    "    pprint.pprint(fieldtypes)\n",
    "\n",
    "    assert fieldtypes[\"areaLand\"] == set([type(1.1), type([]), type(None)])\n",
    "    assert fieldtypes['areaMetro'] == set([type(1.1), type(None)])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'cities.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-0c17d0441bb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-0c17d0441bb6>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCITIES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Printing three example results:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-0c17d0441bb6>\u001b[0m in \u001b[0;36mprocess_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'cities.csv'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up.\n",
    "\n",
    "Since in the previous quiz you made a decision on which value to keep for the\n",
    "\"areaLand\" field, you now know what has to be done.\n",
    "\n",
    "Finish the function fix_area(). It will receive a string as an input, and it\n",
    "has to return a float representing the value of the area or None.\n",
    "You have to change the function fix_area. You can use extra functions if you\n",
    "like, but changes to process_file will not be taken into account.\n",
    "The rest of the code is just an example on how this function can be used.\n",
    "\"\"\"\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "\n",
    "\n",
    "def fix_area(area):\n",
    "    if area=='NULL' or area==\"\":\n",
    "        area = None\n",
    "    elif area[0]=='{':\n",
    "        new_area = area.replace('{','').replace('}', '').strip().split('|')\n",
    "        area1 = new_area[0]\n",
    "        area2 = new_area[1]\n",
    "        if len(area1)>len(area2):\n",
    "            area = float(area1)\n",
    "        else:\n",
    "            area = float(area2)\n",
    "    else:\n",
    "        area =  float(area)\n",
    "    return area\n",
    "\n",
    "\n",
    "\n",
    "def process_file(filename):\n",
    "    # CHANGES TO THIS FUNCTION WILL BE IGNORED WHEN YOU SUBMIT THE EXERCISE\n",
    "    data = []\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            l = reader.next()\n",
    "\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to fix the area value\n",
    "            if \"areaLand\" in line:\n",
    "                line[\"areaLand\"] = fix_area(line[\"areaLand\"])\n",
    "            data.append(line)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = process_file(CITIES)\n",
    "\n",
    "    print \"Printing three example results:\"\n",
    "    for n in range(5,8):\n",
    "        pprint.pprint(data[n][\"areaLand\"])\n",
    "\n",
    "    assert data[3][\"areaLand\"] == None        \n",
    "    assert data[8][\"areaLand\"] == 55166700.0\n",
    "    assert data[20][\"areaLand\"] == 14581600.0\n",
    "    assert data[33][\"areaLand\"] == 20564500.0    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: students",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ab41945e277e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"select name, id from students order by name;\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: students"
     ]
    }
   ],
   "source": [
    "\n",
    "import sqlite3\n",
    "\n",
    "# Fetch some student records from the database.\n",
    "\n",
    "#The connect function for PostgreSQL uses a different argument syntax from the one for SQLite. \n",
    "#When you want to connect to a database called, say, bears, you'll use psycopg2.connect(\"dbname=bears\").\n",
    "\n",
    "db = +.connect(\"students\")\n",
    "c = db.cursor()\n",
    "query = \"select name, id from students order by name;\"\n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "\n",
    "# First, what data structure did we get?\n",
    "print \"Row data:\"\n",
    "print rows\n",
    "\n",
    "# And let's loop over it too:\n",
    "print\n",
    "print \"Student names:\"\n",
    "for row in rows:\n",
    "  print \"  \", row[0]\n",
    "\n",
    "db.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'example.osm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-569b77095638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-569b77095638>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'example.osm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     assert tags == {'bounds': 1,\n",
      "\u001b[0;32m<ipython-input-2-569b77095638>\u001b[0m in \u001b[0;36mcount_tags\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcount_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'example.osm'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task is to use the iterative parsing to process the map file and\n",
    "find out not only what tags are there, but also how many, to get the\n",
    "feeling on how much of which data you can expect to have in the map.\n",
    "Fill out the count_tags function. It should return a dictionary with the \n",
    "tag name as the key and number of times this tag can be encountered in \n",
    "the map as value.\n",
    "\n",
    "Note that your code will be tested with a different data file than the 'example.osm'\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    tree = ET.parse(filename).getroot()\n",
    "    for elem in tree.iter():\n",
    "        if elem.tag in tags:\n",
    "            tags[elem.tag]+=1\n",
    "        else:\n",
    "            tags[elem.tag]=1\n",
    "    return tags\n",
    "        \n",
    "\n",
    "def test():\n",
    "\n",
    "    tags = count_tags('example.osm')\n",
    "    pprint.pprint(tags)\n",
    "    assert tags == {'bounds': 1,\n",
    "                     'member': 3,\n",
    "                     'nd': 4,\n",
    "                     'node': 20,\n",
    "                     'osm': 1,\n",
    "                     'relation': 1,\n",
    "                     'tag': 7,\n",
    "                     'way': 1}\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'example.osm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bab251807354>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-bab251807354>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[1;31m# Note as well that the test function here is only used in the Test Run;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[1;31m# when you submit, your code will be checked against a different dataset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'example.osm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'lower'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lower_colon'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'other'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'problemchars'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-bab251807354>\u001b[0m in \u001b[0;36mprocess_map\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"lower\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lower_colon\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"problemchars\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"other\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, events)\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'example.osm'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\"\"\"\n",
    "Your task is to explore the data a bit more.\n",
    "Before you process the data and add it into your database, you should check the\n",
    "\"k\" value for each \"<tag>\" and see if there are any potential problems.\n",
    "\n",
    "We have provided you with 3 regular expressions to check for certain patterns\n",
    "in the tags. As we saw in the quiz earlier, we would like to change the data\n",
    "model and expand the \"addr:street\" type of keys to a dictionary like this:\n",
    "{\"address\": {\"street\": \"Some value\"}}\n",
    "So, we have to see if we have such tags, and if we have any tags with\n",
    "problematic characters.\n",
    "\n",
    "Please complete the function 'key_type', such that we have a count of each of\n",
    "four tag categories in a dictionary:\n",
    "  \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  \"problemchars\", for tags with problematic characters, and\n",
    "  \"other\", for other tags that do not fall into the other three categories.\n",
    "See the 'process_map' and 'test' functions for examples of the expected format.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        if lower.search(element.attrib['k']):\n",
    "            keys[\"lower\"]+=1\n",
    "        elif lower_colon.search(element.attrib['k']):\n",
    "            keys[\"lower_colon\"]+=1\n",
    "        elif problemchars.search(element.attrib['k']):\n",
    "            keys[\"problemchars\"]+=1\n",
    "        else:\n",
    "             keys[\"other\"]+=1\n",
    "            \n",
    "        pass\n",
    "        \n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "def test():\n",
    "    # You can use another testfile 'map.osm' to look at your solution\n",
    "    # Note that the assertion below will be incorrect then.\n",
    "    # Note as well that the test function here is only used in the Test Run;\n",
    "    # when you submit, your code will be checked against a different dataset.\n",
    "    keys = process_map('example.osm')\n",
    "    pprint.pprint(keys)\n",
    "    assert keys == {'lower': 5, 'lower_colon': 0, 'other': 1, 'problemchars': 1}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'example.osm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2c1239d73deb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-2c1239d73deb>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0musers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'example.osm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2c1239d73deb>\u001b[0m in \u001b[0;36mprocess_map\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0musers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'uid'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrib\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0musers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrib\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'uid'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, events)\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'example.osm'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\"\"\"\n",
    "Your task is to explore the data a bit more.\n",
    "The first task is a fun one - find out how many unique users\n",
    "have contributed to the map in this particular area!\n",
    "\n",
    "The function process_map should return a set of unique user IDs (\"uid\")\n",
    "\"\"\"\n",
    "\n",
    "def get_user(element):\n",
    "    return\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for event, element in ET.iterparse(filename):\n",
    "        if 'uid' in element.attrib:\n",
    "            users.add(element.attrib['uid'])\n",
    "    return users\n",
    "\n",
    "def test():\n",
    "\n",
    "    users = process_map('example.osm')\n",
    "    pprint.pprint(users)\n",
    "    assert len(users) == 6\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database.\n",
    "To do so you will parse the elements in the OSM XML file, transforming them from document format to\n",
    "tabular format, thus making it possible to write to .csv files.  These csv files can then easily be\n",
    "imported to a SQL database as tables.\n",
    "\n",
    "The process for this transformation is as follows:\n",
    "- Use iterparse to iteratively step through each top level element in the XML\n",
    "- Shape each element into several data structures using a custom function\n",
    "- Utilize a schema and validation library to ensure the transformed data is in the correct format\n",
    "- Write each data structure to the appropriate .csv files\n",
    "\n",
    "We've already provided the code needed to load the data, perform iterative parsing and write the\n",
    "output to csv files. Your task is to complete the shape_element function that will transform each\n",
    "element into the correct format. To make this process easier we've already defined a schema (see\n",
    "the schema.py file in the last code tab) for the .csv files and the eventual tables. Using the \n",
    "cerberus library we can validate the output against this schema to ensure it is correct.\n",
    "\n",
    "## Shape Element Function\n",
    "The function should take as input an iterparse Element object and return a dictionary.\n",
    "\n",
    "### If the element top level tag is \"node\":\n",
    "The dictionary returned should have the format {\"node\": .., \"node_tags\": ...}\n",
    "\n",
    "The \"node\" field should hold a dictionary of the following top level node attributes:\n",
    "- id\n",
    "- user\n",
    "- uid\n",
    "- version\n",
    "- lat\n",
    "- lon\n",
    "- timestamp\n",
    "- changeset\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"node_tags\" field should hold a list of dictionaries, one per secondary tag. Secondary tags are\n",
    "child tags of node which have the tag name/type: \"tag\". Each dictionary should have the following\n",
    "fields from the secondary tag attributes:\n",
    "- id: the top level node id attribute value\n",
    "- key: the full tag \"k\" attribute value if no colon is present or the characters after the colon if one is.\n",
    "- value: the tag \"v\" attribute value\n",
    "- type: either the characters before the colon in the tag \"k\" value or \"regular\" if a colon\n",
    "        is not present.\n",
    "\n",
    "Additionally,\n",
    "\n",
    "- if the tag \"k\" value contains problematic characters, the tag should be ignored\n",
    "- if the tag \"k\" value contains a \":\" the characters before the \":\" should be set as the tag type\n",
    "  and characters after the \":\" should be set as the tag key\n",
    "- if there are additional \":\" in the \"k\" value they and they should be ignored and kept as part of\n",
    "  the tag key. For example:\n",
    "\n",
    "  <tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "  should be turned into\n",
    "  {'id': 12345, 'key': 'street:name', 'value': 'Lincoln', 'type': 'addr'}\n",
    "\n",
    "- If a node has no secondary tags then the \"node_tags\" field should just contain an empty list.\n",
    "\n",
    "The final return value for a \"node\" element should look something like:\n",
    "\n",
    "{'node': {'id': 757860928,\n",
    "          'user': 'uboot',\n",
    "          'uid': 26299,\n",
    "       'version': '2',\n",
    "          'lat': 41.9747374,\n",
    "          'lon': -87.6920102,\n",
    "          'timestamp': '2010-07-22T16:16:51Z',\n",
    "      'changeset': 5288876},\n",
    " 'node_tags': [{'id': 757860928,\n",
    "                'key': 'amenity',\n",
    "                'value': 'fast_food',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'cuisine',\n",
    "                'value': 'sausage',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'name',\n",
    "                'value': \"Shelly's Tasty Freeze\",\n",
    "                'type': 'regular'}]}\n",
    "\n",
    "### If the element top level tag is \"way\":\n",
    "The dictionary should have the format {\"way\": ..., \"way_tags\": ..., \"way_nodes\": ...}\n",
    "\n",
    "The \"way\" field should hold a dictionary of the following top level way attributes:\n",
    "- id\n",
    "-  user\n",
    "- uid\n",
    "- version\n",
    "- timestamp\n",
    "- changeset\n",
    "\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"way_tags\" field should again hold a list of dictionaries, following the exact same rules as\n",
    "for \"node_tags\".\n",
    "\n",
    "Additionally, the dictionary should have a field \"way_nodes\". \"way_nodes\" should hold a list of\n",
    "dictionaries, one for each nd child tag.  Each dictionary should have the fields:\n",
    "- id: the top level element (way) id\n",
    "- node_id: the ref attribute value of the nd tag\n",
    "- position: the index starting at 0 of the nd tag i.e. what order the nd tag appears within\n",
    "            the way element\n",
    "\n",
    "The final return value for a \"way\" element should look something like:\n",
    "\n",
    "{'way': {'id': 209809850,\n",
    "         'user': 'chicago-buildings',\n",
    "         'uid': 674454,\n",
    "         'version': '1',\n",
    "         'timestamp': '2013-03-13T15:58:04Z',\n",
    "         'changeset': 15353317},\n",
    " 'way_nodes': [{'id': 209809850, 'node_id': 2199822281, 'position': 0},\n",
    "               {'id': 209809850, 'node_id': 2199822390, 'position': 1},\n",
    "               {'id': 209809850, 'node_id': 2199822392, 'position': 2},\n",
    "               {'id': 209809850, 'node_id': 2199822369, 'position': 3},\n",
    "               {'id': 209809850, 'node_id': 2199822370, 'position': 4},\n",
    "               {'id': 209809850, 'node_id': 2199822284, 'position': 5},\n",
    "               {'id': 209809850, 'node_id': 2199822281, 'position': 6}],\n",
    " 'way_tags': [{'id': 209809850,\n",
    "               'key': 'housenumber',\n",
    "               'type': 'addr',\n",
    "               'value': '1412'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street',\n",
    "               'type': 'addr',\n",
    "               'value': 'West Lexington St.'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:name',\n",
    "               'type': 'addr',\n",
    "               'value': 'Lexington'},\n",
    "              {'id': '209809850',\n",
    "               'key': 'street:prefix',\n",
    "               'type': 'addr',\n",
    "               'value': 'West'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:type',\n",
    "               'type': 'addr',\n",
    "               'value': 'Street'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'building',\n",
    "               'type': 'regular',\n",
    "               'value': 'yes'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'levels',\n",
    "               'type': 'building',\n",
    "               'value': '1'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'building_id',\n",
    "               'type': 'chicago',\n",
    "               'value': '366409'}]}\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "OSM_PATH = \"example.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def get_tags(element):\n",
    "    tags = []\n",
    "    for i in element.iter(\"tag\"):\n",
    "        tag={}\n",
    "        tag[\"id\"] =  element.attrib[\"id\"]\n",
    "        if PROBLEMCHARS.search(i.attrib['k']):\n",
    "            continue\n",
    "        elif \":\" in i.attrib['k']:\n",
    "            splitted =  i.attrib['k'].split(':')\n",
    "            tag[\"type\"] = splitted[0]\n",
    "            if len(splitted)==3:\n",
    "                tag[\"key\"] = splitted[1]+\":\"+splitted[2]\n",
    "            else:\n",
    "                tag[\"key\"] = splitted[1]\n",
    "        else:\n",
    "            tag[\"type\"] = \"regular\"\n",
    "            tag[\"key\"] = i.attrib['k']\n",
    "            \n",
    "        tag[\"value\"] = i.attrib[\"v\"]\n",
    "        tags.append(tag)\n",
    "    return tags\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if element.tag == 'node':\n",
    "        node_attribs[\"id\"] = element.attrib[\"id\"]\n",
    "        node_attribs[\"lat\"] =  element.attrib['lat']\n",
    "        node_attribs[\"lon\"] =  element.attrib['lon']\n",
    "        node_attribs[\"user\"] =  element.attrib['user']\n",
    "        node_attribs[\"uid\"] =  element.attrib['uid']\n",
    "        node_attribs[\"version\"] =  element.attrib['version']\n",
    "        node_attribs[\"changeset\"] =  element.attrib['changeset']\n",
    "        node_attribs[\"timestamp\"] =  element.attrib['timestamp']\n",
    "        \n",
    "        tags = get_tags(element)\n",
    "       \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "        \n",
    "    elif element.tag == 'way':\n",
    "        way_attribs[\"id\"] = element.attrib[\"id\"]\n",
    "        way_attribs[\"user\"] =  element.attrib['user']\n",
    "        way_attribs[\"uid\"] =  element.attrib['uid']\n",
    "        way_attribs[\"version\"] =  element.attrib['version']\n",
    "        way_attribs[\"changeset\"] =  element.attrib['changeset']\n",
    "        way_attribs[\"timestamp\"] =  element.attrib['timestamp']\n",
    "        \n",
    "        tags = get_tags(element)\n",
    "        \n",
    "        nd_count=0\n",
    "        for i in element.iter(\"nd\"):\n",
    "            nd={}\n",
    "            nd[\"id\"] =  element.attrib[\"id\"]\n",
    "            nd[\"node_id\"] =  i.attrib[\"ref\"]\n",
    "            nd[\"position\"] =  nd_count\n",
    "            way_nodes.append(nd)\n",
    "            nd_count+=1\n",
    "        \n",
    "\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'arachnid.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d06b0ad8617b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-d06b0ad8617b>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATAFILE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFIELDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Your first entry:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d06b0ad8617b>\u001b[0m in \u001b[0;36mprocess_file\u001b[0;34m(filename, fields)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mprocess_fields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'arachnid.csv'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with another type of infobox data, audit it,\n",
    "clean it, come up with a data model, insert it into MongoDB and then run some\n",
    "queries against your database. The set contains data about Arachnid class\n",
    "animals.\n",
    "\n",
    "Your task in this exercise is to parse the file, process only the fields that\n",
    "are listed in the FIELDS dictionary as keys, and return a list of dictionaries\n",
    "of cleaned values. \n",
    "\n",
    "The following things should be done:\n",
    "- keys of the dictionary changed according to the mapping in FIELDS dictionary\n",
    "- trim out redundant description in parenthesis from the 'rdf-schema#label'\n",
    "  field, like \"(spider)\"\n",
    "- if 'name' is \"NULL\" or contains non-alphanumeric characters, set it to the\n",
    "  same value as 'label'.\n",
    "- if a value of a field is \"NULL\", convert it to None\n",
    "- if there is a value in 'synonym', it should be converted to an array (list)\n",
    "  by stripping the \"{}\" characters and splitting the string on \"|\". Rest of the\n",
    "  cleanup is up to you, e.g. removing \"*\" prefixes etc. If there is a singular\n",
    "  synonym, the value should still be formatted in a list.\n",
    "- strip leading and ending whitespace from all fields, if there is any\n",
    "- the output structure should be as follows:\n",
    "\n",
    "[ { 'label': 'Argiope',\n",
    "    'uri': 'http://dbpedia.org/resource/Argiope_(spider)',\n",
    "    'description': 'The genus Argiope includes rather large and spectacular spiders that often ...',\n",
    "    'name': 'Argiope',\n",
    "    'synonym': [\"One\", \"Two\"],\n",
    "    'classification': {\n",
    "                      'family': 'Orb-weaver spider',\n",
    "                      'class': 'Arachnid',\n",
    "                      'phylum': 'Arthropod',\n",
    "                      'order': 'Spider',\n",
    "                      'kingdom': 'Animal',\n",
    "                      'genus': None\n",
    "                      }\n",
    "  },\n",
    "  { 'label': ... , }, ...\n",
    "]\n",
    "\n",
    "  * Note that the value associated with the classification key is a dictionary\n",
    "    with taxonomic labels.\n",
    "\"\"\"\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "DATAFILE = 'arachnid.csv'\n",
    "FIELDS ={'rdf-schema#label': 'label',\n",
    "         'URI': 'uri',\n",
    "         'rdf-schema#comment': 'description',\n",
    "         'synonym': 'synonym',\n",
    "         'name': 'name',\n",
    "         'family_label': 'family',\n",
    "         'class_label': 'class',\n",
    "         'phylum_label': 'phylum',\n",
    "         'order_label': 'order',\n",
    "         'kingdom_label': 'kingdom',\n",
    "         'genus_label': 'genus'}\n",
    "\n",
    "\n",
    "def process_file(filename, fields):\n",
    "\n",
    "    process_fields = fields.keys()\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i in range(3):\n",
    "            l = reader.next()\n",
    "        \n",
    "        for line in reader:\n",
    "            row={}\n",
    "            # YOUR CODE HERE\n",
    "            row['classification'] = {}\n",
    "            for field in fields:\n",
    "                content = line[field]\n",
    "                if field ==\"rdf-schema#label\" and \"(\" in content :\n",
    "                    content = content[:content.find(\"(\")].strip()\n",
    "                    \n",
    "                elif field ==\"name\" and (content == \"NULL\" or re.match('^[\\w-]+$', content) is None):\n",
    "                    content = None\n",
    "                elif content==\"NULL\":\n",
    "                    content=None\n",
    "                elif field=='synonym' and content is not None:\n",
    "                    if '{' in content:\n",
    "                        content = content.replace('{','').replace('}', '').replace('*', '').strip().split('|')\n",
    "                    else:\n",
    "                        content = [content.strip()]\n",
    "                else:\n",
    "                    content=content.strip()\n",
    "\n",
    "                if fields[field] in ['family', 'class', 'phylum','order', 'kingdom','genus']:\n",
    "                    row['classification'][fields[field]]=content\n",
    "                else:\n",
    "                    row[fields[field]]=content\n",
    "                    \n",
    "            if row['name'] == None:\n",
    "                row['name'] = row['label']\n",
    "                \n",
    "            data.append(row)\n",
    "            pass\n",
    "    print data\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_array(v):\n",
    "    if (v[0] == \"{\") and (v[-1] == \"}\"):\n",
    "        v = v.lstrip(\"{\")\n",
    "        v = v.rstrip(\"}\")\n",
    "        v_array = v.split(\"|\")\n",
    "        v_array = [i.strip() for i in v_array]\n",
    "        return v_array\n",
    "    return [v]\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = process_file(DATAFILE, FIELDS)\n",
    "    print \"Your first entry:\"\n",
    "    pprint.pprint(data[0])\n",
    "    first_entry = {\n",
    "        \"synonym\": None, \n",
    "        \"name\": \"Argiope\", \n",
    "        \"classification\": {\n",
    "            \"kingdom\": \"Animal\", \n",
    "            \"family\": \"Orb-weaver spider\", \n",
    "            \"order\": \"Spider\", \n",
    "            \"phylum\": \"Arthropod\", \n",
    "            \"genus\": None, \n",
    "            \"class\": \"Arachnid\"\n",
    "        }, \n",
    "        \"uri\": \"http://dbpedia.org/resource/Argiope_(spider)\", \n",
    "        \"label\": \"Argiope\", \n",
    "        \"description\": \"The genus Argiope includes rather large and spectacular spiders that often have a strikingly coloured abdomen. These spiders are distributed throughout the world. Most countries in tropical or temperate climates host one or more species that are similar in appearance. The etymology of the name is from a Greek name meaning silver-faced.\"\n",
    "    }\n",
    "\n",
    "    assert len(data) == 76\n",
    "    assert data[0] == first_entry\n",
    "    assert data[17][\"name\"] == \"Ogdenia\"\n",
    "    assert data[48][\"label\"] == \"Hydrachnidiae\"\n",
    "    assert data[14][\"synonym\"] == [\"Cyrene Peckham & Peckham\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "localhost:27017: [Errno 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-59298f797dbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'twitter'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet_sources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-59298f797dbb>\u001b[0m in \u001b[0;36mtweet_sources\u001b[0;34m(db, pipeline)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtweet_sources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\collection.pyc\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, pipeline, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[1;31m# If the server does not support the \"cursor\" option we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m         \u001b[1;31m# ignore useCursor and batchSize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket_for_reads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslave_ok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_wire_version\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0muse_cursor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\mongo_client.pyc\u001b[0m in \u001b[0;36m_socket_for_reads\u001b[0;34m(self, read_preference)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mtopology\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_topology\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0msingle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopology_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTOPOLOGY_TYPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSingle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_preference\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m             slave_ok = (single and not sock_info.is_mongos) or (\n\u001b[1;32m    800\u001b[0m                 preference != ReadPreference.PRIMARY)\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\mongo_client.pyc\u001b[0m in \u001b[0;36m_get_socket\u001b[0;34m(self, selector)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mserver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_topology\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_server\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__all_credentials\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\topology.pyc\u001b[0m in \u001b[0;36mselect_server\u001b[0;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[1;32m    208\u001b[0m         return random.choice(self.select_servers(selector,\n\u001b[1;32m    209\u001b[0m                                                  \u001b[0mserver_selection_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                                                  address))\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     def select_server_by_address(self, address,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\topology.pyc\u001b[0m in \u001b[0;36mselect_servers\u001b[0;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mserver_timeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnow\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                     raise ServerSelectionTimeoutError(\n\u001b[0;32m--> 186\u001b[0;31m                         self._error_message(selector))\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_opened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m: localhost:27017: [Errno 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "The tweets in our twitter collection have a field called \"source\". This field describes the application\n",
    "that was used to create the tweet. Following the examples for using the $group operator, your task is \n",
    "to modify the 'make-pipeline' function to identify most used applications for creating tweets. \n",
    "As a check on your query, 'web' is listed as the most frequently used application.\n",
    "'Ubertwitter' is the second most used. The number of counts should be stored in a field named 'count'\n",
    "(see the assertion at the end of the script).\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation pipeline\n",
    "that can be passed to the MongoDB aggregate function. As in our examples in this lesson, the aggregation \n",
    "pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. \n",
    "If you want to run this code locally on your machine, you have to install MongoDB, \n",
    "download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset \n",
    "used in examples in this lesson. \n",
    "If you attempt some of the same queries that we looked at in the lesson examples,\n",
    "your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [{\"$group\": {\"_id\": \"$source\",\n",
    "                            \"count\": {\"$sum\":1}}},\n",
    "                {\"$sort\":{\"count\":-1}}\n",
    "                ]\n",
    "    return pipeline\n",
    "\n",
    "def tweet_sources(db, pipeline):\n",
    "    return [doc for doc in db.tweets.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('twitter')\n",
    "    pipeline = make_pipeline()\n",
    "    result = tweet_sources(db, pipeline)\n",
    "    import pprint\n",
    "    pprint.pprint(result[0])\n",
    "    assert result[0] == {u'count': 868, u'_id': u'web'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "localhost:27017: [Errno 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a496f0202a9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'twitter'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a496f0202a9d>\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(db, pipeline)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\collection.pyc\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, pipeline, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[1;31m# If the server does not support the \"cursor\" option we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m         \u001b[1;31m# ignore useCursor and batchSize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket_for_reads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslave_ok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_wire_version\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0muse_cursor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\mongo_client.pyc\u001b[0m in \u001b[0;36m_socket_for_reads\u001b[0;34m(self, read_preference)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mtopology\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_topology\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0msingle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopology_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTOPOLOGY_TYPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSingle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_preference\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m             slave_ok = (single and not sock_info.is_mongos) or (\n\u001b[1;32m    800\u001b[0m                 preference != ReadPreference.PRIMARY)\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\mongo_client.pyc\u001b[0m in \u001b[0;36m_get_socket\u001b[0;34m(self, selector)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mserver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_topology\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_server\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__all_credentials\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\topology.pyc\u001b[0m in \u001b[0;36mselect_server\u001b[0;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[1;32m    208\u001b[0m         return random.choice(self.select_servers(selector,\n\u001b[1;32m    209\u001b[0m                                                  \u001b[0mserver_selection_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                                                  address))\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     def select_server_by_address(self, address,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\topology.pyc\u001b[0m in \u001b[0;36mselect_servers\u001b[0;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mserver_timeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnow\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                     raise ServerSelectionTimeoutError(\n\u001b[0;32m--> 186\u001b[0;31m                         self._error_message(selector))\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_opened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m: localhost:27017: [Errno 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Write an aggregation query to answer this question:\n",
    "\n",
    "Of the users in the \"Brasilia\" timezone who have tweeted 100 times or more,\n",
    "who has the largest number of followers?\n",
    "\n",
    "The following hints will help you solve this problem:\n",
    "- Time zone is found in the \"time_zone\" field of the user object in each tweet.\n",
    "- The number of tweets for each user is found in the \"statuses_count\" field.\n",
    "  To access these fields you will need to use dot notation (from Lesson 4)\n",
    "- Your aggregation query should return something like the following:\n",
    "{u'ok': 1.0,\n",
    " u'result': [{u'_id': ObjectId('52fd2490bac3fa1975477702'),\n",
    "                  u'followers': 2597,\n",
    "                  u'screen_name': u'marbles',\n",
    "                  u'tweets': 12334}]}\n",
    "Note that you will need to create the fields 'followers', 'screen_name' and 'tweets'.\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation \n",
    "pipeline that can be passed to the MongoDB aggregate function. As in our examples in this lesson,\n",
    "the aggregation pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code\n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset used \n",
    "in examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [\n",
    "        {\"$match\" : { \"user.time_zone\": \"Brasilia\" ,\n",
    "                    \"user.statuses_count\" : { \"$gte\":100}}},\n",
    "        {\"$project\": {\"followers\" : \"$user.followers_count\",\n",
    "                        \"screen_name\" : \"$user.screen_name\",\n",
    "                        \"tweets\": \"$user.statuses_count\"}},\n",
    "        {\"$sort\": {\"followers\": -1}},\n",
    "        {\"$limit\": 1}]\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.tweets.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('twitter')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    import pprint\n",
    "    pprint.pprint(result)\n",
    "    assert len(result) == 1\n",
    "    assert result[0][\"followers\"] == 17209\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "localhost:27017: [Errno 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bba332ada81d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'examples'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Printing the first result:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-bba332ada81d>\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(db, pipeline)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\collection.pyc\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, pipeline, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[1;31m# If the server does not support the \"cursor\" option we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m         \u001b[1;31m# ignore useCursor and batchSize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket_for_reads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslave_ok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_wire_version\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0muse_cursor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\mongo_client.pyc\u001b[0m in \u001b[0;36m_socket_for_reads\u001b[0;34m(self, read_preference)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mtopology\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_topology\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0msingle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopology_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTOPOLOGY_TYPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSingle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_preference\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m             slave_ok = (single and not sock_info.is_mongos) or (\n\u001b[1;32m    800\u001b[0m                 preference != ReadPreference.PRIMARY)\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\mongo_client.pyc\u001b[0m in \u001b[0;36m_get_socket\u001b[0;34m(self, selector)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mserver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_topology\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_server\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__all_credentials\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\topology.pyc\u001b[0m in \u001b[0;36mselect_server\u001b[0;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[1;32m    208\u001b[0m         return random.choice(self.select_servers(selector,\n\u001b[1;32m    209\u001b[0m                                                  \u001b[0mserver_selection_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                                                  address))\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     def select_server_by_address(self, address,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\topology.pyc\u001b[0m in \u001b[0;36mselect_servers\u001b[0;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mserver_timeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnow\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                     raise ServerSelectionTimeoutError(\n\u001b[0;32m--> 186\u001b[0;31m                         self._error_message(selector))\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_opened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m: localhost:27017: [Errno 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "For this exercise, let's return to our cities infobox dataset. The question we would like you to answer\n",
    "is as follows:  Which region or district in India contains the most cities? (Make sure that the count of\n",
    "cities is stored in a field named 'count'; see the assertions at the end of the script.)\n",
    "\n",
    "As a starting point, use the solution for the example question we looked at -- \"Who includes the most\n",
    "user mentions in their tweets?\"\n",
    "\n",
    "One thing to note about the cities data is that the \"isPartOf\" field contains an array of regions or \n",
    "districts in which a given city is found. See the example document in Instructor Comments below.\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation pipeline \n",
    "that can be passed to the MongoDB aggregate function. As in our examples in this lesson, the aggregation \n",
    "pipeline should be a list of one or more dictionary objects. Please review the lesson examples if you \n",
    "are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code \n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the cities collection used in \n",
    "examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results may be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline =  [\n",
    "         {\"$match\" : { \"country\": \"India\" }},\n",
    "        {\"$unwind\" : \"$isPartOf\"},\n",
    "        {\"$group\" : {\"_id\" : \"$isPartOf\",\n",
    "                        \"count\": {\"$sum\":1}}},\n",
    "         {\"$sort\": {\"count\": -1}},\n",
    "        {\"$limit\": 1}]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.cities.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('examples')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    print \"Printing the first result:\"\n",
    "    import pprint\n",
    "    pprint.pprint(result[0])\n",
    "    assert result[0][\"_id\"] == \"Uttar Pradesh\"\n",
    "    assert result[0][\"count\"] == 623\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "localhost:27017: [Errno 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2033aff0626d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'twitter'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2033aff0626d>\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(db, pipeline)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\collection.pyc\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, pipeline, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[1;31m# If the server does not support the \"cursor\" option we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m         \u001b[1;31m# ignore useCursor and batchSize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket_for_reads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslave_ok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_wire_version\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0muse_cursor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\mongo_client.pyc\u001b[0m in \u001b[0;36m_socket_for_reads\u001b[0;34m(self, read_preference)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mtopology\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_topology\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0msingle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopology_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTOPOLOGY_TYPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSingle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_preference\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m             slave_ok = (single and not sock_info.is_mongos) or (\n\u001b[1;32m    800\u001b[0m                 preference != ReadPreference.PRIMARY)\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\mongo_client.pyc\u001b[0m in \u001b[0;36m_get_socket\u001b[0;34m(self, selector)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mserver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_topology\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_server\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__all_credentials\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\topology.pyc\u001b[0m in \u001b[0;36mselect_server\u001b[0;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[1;32m    208\u001b[0m         return random.choice(self.select_servers(selector,\n\u001b[1;32m    209\u001b[0m                                                  \u001b[0mserver_selection_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                                                  address))\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     def select_server_by_address(self, address,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pymongo\\topology.pyc\u001b[0m in \u001b[0;36mselect_servers\u001b[0;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mserver_timeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnow\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                     raise ServerSelectionTimeoutError(\n\u001b[0;32m--> 186\u001b[0;31m                         self._error_message(selector))\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_opened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m: localhost:27017: [Errno 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "$push is similar to $addToSet. The difference is that rather than accumulating only unique values \n",
    "it aggregates all values into an array.\n",
    "\n",
    "Using an aggregation query, count the number of tweets for each user. In the same $group stage, \n",
    "use $push to accumulate all the tweet texts for each user. Limit your output to the 5 users\n",
    "with the most tweets. \n",
    "Your result documents should include only the fields:\n",
    "\"_id\" (screen name of user), \n",
    "\"count\" (number of tweets found for the user),\n",
    "\"tweet_texts\" (a list of the tweet texts found for the user).  \n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation \n",
    "pipeline that can be passed to the MongoDB aggregate function. As in our examples in this lesson, \n",
    "the aggregation pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code \n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset used in \n",
    "examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [\n",
    "        {\"$group\" : { \"_id\": \"$user.screen_name\",\n",
    "                    \"count\":{\"$sum\":1},\n",
    "                    \"tweet_texts\": { \"$push\": \"$text\"}}\n",
    "        },\n",
    "        \n",
    "        {\"$sort\": {\"count\": -1}},\n",
    "        {\"$limit\": 5}]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.twitter.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('twitter')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    import pprint\n",
    "    pprint.pprint(result)\n",
    "    assert len(result) == 5\n",
    "    assert result[0][\"count\"] > result[4][\"count\"]\n",
    "    sample_tweet_text = u'Take my money! #liesguystell http://movie.sras2.ayorganes.com'\n",
    "    assert result[4][\"tweet_texts\"][0] == sample_tweet_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "In an earlier exercise we looked at the cities dataset and asked which region in India contains \n",
    "the most cities. In this exercise, we'd like you to answer a related question regarding regions in \n",
    "India. What is the average city population for a region in India? Calculate your answer by first \n",
    "finding the average population of cities in each region and then by calculating the average of the \n",
    "regional averages.\n",
    "\n",
    "Hint: If you want to accumulate using values from all input documents to a group stage, you may use \n",
    "a constant as the value of the \"_id\" field. For example, \n",
    "    { \"$group\" : {\"_id\" : \"India Regional City Population Average\",\n",
    "      ... }\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation \n",
    "pipeline that can be passed to the MongoDB aggregate function. As in our examples in this lesson, \n",
    "the aggregation pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code \n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset used \n",
    "in examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [\n",
    "         {\"$match\" : { \"country\": \"India\" }},\n",
    "        {\"$unwind\" : \"$isPartOf\"},\n",
    "        {\"$group\" : {\"_id\" : \"$isPartOf\",\n",
    "                        \"avg\": {\"$avg\":\"$population\"}}},\n",
    "        {\"$group\" : {\"_id\" : \"India Regional City Population Average\",\n",
    "            \"avg\": {\"$avg\" : \"$avg\"}}}]\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.cities.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('examples')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    assert len(result) == 1\n",
    "    # Your result should be close to the value after the minus sign.\n",
    "    assert abs(result[0][\"avg\"] - 201128.0241546919) < 10 ** -8\n",
    "    import pprint\n",
    "    pprint.pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
